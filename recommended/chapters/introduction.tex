\chapter{Introduction}
\section{Overview}


\epigraph{\textit{No one knows what the right algorithm is, but it gives us hope that if we can discover some crude approximation of whatever this algorithm is and implement it on a computer, that can help us make a lot of progress.}}{\rightline{{\rm --- Andrew Ng}}}

Every time we are interacting with our environment, we get closer to understanding it. %still the closer we are the less we understand. 
However, as a byproduct, further questions arise, for which pure logical (or mathematical) solution is not available, or the problem is larger what we can solve.
Luckily we can always turn back to nature for inspiration: biological systems have proven their efficiency, therefore their functions are worth to be further analyzed -- even if there could be theoretically a better way to tackle obstacles. 
One of the many branches of artificial intelligence, Neural Networks are based on the nervous systems of living organisms which is capable of self-learning.

Such a simple paradigm is playing a fundamental role in boosting 
industry and research of today, because introducing Machine Learning to many fields of life seems to results in great leap forward. 
Thanks to the recent technological advancements, even with the current computational capacity of a personal computer one can get encouraging results by simply exploring the core principles of the topic. 

In this work I provide a detailed explanation of each step of building a library,
which is capable of reproducing current results of scientific researches.
Also present applications which were made with my own library and case studies in which I have achieved unique results.
Finally I propose the main innovation of my recent work: a simple algorithm for visualizing the \textbf{perceptive field} of each layer in a feed-forward network.

\paragraph{Primary goal.} With my work I intend to bring closer, and demystify cutting-edge concepts of applied neural networkings for larger audiences. 
On related works and case studies I want to show what can be done by simply going back to the drawing board


\section{Related Works and Literature}

\paragraph{Main motivation.} By using external libraries \cite{TF, torch, caffe}, without diving deep into mathematical proofs, and treating neural networks as black-boxes, thousands of useful applications \cite{haykin2004comprehensive} are made. 
On the other hand, building such architectures from the very basics helps to clarify how simple units might be organized~\cite{milo2002network}, taught \cite{werbos1994roots}, and function \cite{hornik1989multilayer} as a large system. 
As complexity arises, the processes in the network becomes unclear and brings the question: what is the purpose of each node? 
Methods to visualize how activation patterns formulate, and what information is held within them has been already investigated \cite{yosinski2015understanding}, and applied to improve performance \cite{zeiler2014visualizing}. 
My studies mainly rely on recent publications, and research in field of computer vision. The design of my own Deep Learning framework library is influenced by off-the-press tutorials \cite{Goodfellow-et-al-2016-Book, deeplearningdotnet, nnsdl, stanfordlectures, gibiansky} and open-source libraries \cite{TF, torch, caffe} available to anyone.

Considering course slides \cite{stanfordlectures, oxfordlectures} and textbooks \cite{Goodfellow-et-al-2016-Book, werbos1994roots, bengio2009learning} of established universities, popular websites \cite{deeplearningdotnet, pedregosa2011scikit}, 
articles \cite{lecun2015deep}, blogs \cite{gibiansky, karpathyblog} and vlogs \cite{vlog1} on Artificial Neural Networks one might find it hard to find a good point to start. 
Some content may offer formal description of the general machine learning problem, others try to clarify through analogies with the biological nerve system.
Tutorials, which I have found interesting, and the most helpful, had several common features which are important to adapt, when designing a neural network library.
Generally these common attributes are the following: 
they intend to be \emph{simple} as possible, 
have many \emph{intuitive} examples and analogies,
their \emph{interpretations} are not restricted to either universal approximators, 
or nervous systems of living organisms, but combines both aspect.

I have written the first implementation based on chapters of \cite{nnsdl}. 
What I have understood is the basic forward- and back-propagation \cite{werbos1994roots} concept of neural networks.
Since I was struggling to understand the coding tricks that the author involved 
I could not extend the source to any problem.
I decided to work out the mathematical formulation, to re-implement feed-forward networks from the beginning.
Reading a practical guide~\cite{karpathyblog} pointed out how can we represent \emph{gradient flow}, \emph{computational graphs} in a simple way.
His style of explaining complex theoretical concepts through very clear examples motivated me to
write my own guide which covers those parts that I was missing from the literature mentioned above.



\section{Thesis outline and contributions}






%
%\section*{Biological Motivation}
%\section*{Real Life Applications}
%\section*{Black Box representation}
%\section{The Perceptron model}
%\section{Importance of understanding}
%\section{Visualization}
