\chapter{Summary}

In this work I provide a detailed introduction of building feed-forward neural networks, supported by both illustrative examples and mathematical proofs. 
The examples are based on biological motivations, and practical applications of NN. 
In the \textbf{Design and Implementation} chapter I present a guide to understanding and writing the neural network framework step-by-step, which I used to produce results in \textbf{Case Studies}. 
The newest implementation can be find at \cite{DV}, supported with out-of-the-box demo scripts, and \texttt{ipython notebooks}.
The library is modular in the first place, and easy to utilize because of the layer manager functions of the \texttt{network} class found in the \url{network_module}. 
It is also easy to \emph{fork} the implementation, and join the further development of the framework -- the phases of improvement are well documented, thanks to the version control supported by Git, and synchronized on the GitHub repository.

By working on this project I have understood the main concepts of Machine Learning, and experimented with multiple design patterns on how to realize the inference \ref{eq:forward} and backpropagation \ref{backprop} functions and arrived at the best fitting design: 
object-oriented implementation of \textbf{layers} handling the lattices of the computational graph derived from the abstract layer and \textbf{networks} managing the organization and training of layers.
My implementation offers both user-friendly interface and opportunity to fine-tune the network with advanced parameters as optional arguments.

I have utilized and tested the network on different tasks of popular Machine Learning problems, namely \textbf{Voice Recognition}, and \textbf{Image Classification}.
I was not just able to reproduce results of other researchers but I have also produced unique results;
while studying novel techniques of revealing perceptive fields and favored patterns of neurons, 
I have developed my own method for visualization by altering the backpropagation function of layers, arriving at the biased and unbiased \textbf{Gradient Ascent}.
I made experiments on the effects of GA, and studied how different shaped networks infer the input data, also how they could be exploited to make adversarial inputs. In a nutshell the results are the following: unbiased GA should be used for transforming samples into inputs which fools classifying networks, and biased GA for enhancing multiple samples to extract patterns from neural networks. The details of the experiments are explained in \textbf{\nameref{sec:MNIST}} section.
