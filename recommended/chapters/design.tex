\chapter{Design and Implementation}

After researching literature on building libraries from scratch, and analyzing pet project source-codes \cite{convnetjs, gibianskysource} and implementations of professional libraries \cite{TF, caffe, torch}
I concluded that the best way to acquire an in-dept understanding of neural networks is to build my own Deep Learning framework.
This way I had a chance to understand why novel solutions in Machine Learning are formed the way they are.
I also gained insight into what main paradigms are popular libraries based on, such as \emph{computational graphs, parallel processing}, 
and what trade-offs can be made between \emph{computational cost} and \emph{memory usage}, between robustness and plasticity.
Most thankfully, by starting from scratch I have faced situations when theoretical formulas had to be translated into exact working code, which was an important challenge.

\paragraph{Goals.} My main objectives when writing code was the following:
\begin{itemize}
    \item[] to make such a library that is able to be extended further
    \item[] open-source, so it can be forked by anyone interested in developing it
    \item[] to make it modular therefore make its usage independent of the task
    \item[] use the fewest possible technical tricks for sake of simplicity
    \item[] to stay as close to pure mathematical formulation of the classical paradigms as possible
    \item[] put emphasis on ease of use and understanding
\end{itemize}
\paragraph{Disclaimer.} Apart from \textbf{NumPy} and its complementary package \textbf{SciPy}, no external libraries and dependencies are used the implementation. 
I want to emphasize that this work was not written to compete with contemporary state-of-the-art frameworks, rather to help perceive the general ideas behind novel researches, and to encourage interested fellows to carry out researches on their own.

\section{Choice of design pattern and language}

The formulas of classical neural networks, whose nodes are organized into lattices forming a Directed Acyclic Graph, shows minor variance in contents of independent sources. Still their common point that they rely on basic vector algebra and functional analysis, therefore considering array represented general mapping functions, as their core building units is a paradigm which would not interfere with the current formal and informal descriptions of deep learning.

\subsection{Disassembling a universal approximator}
Thinking of a neural net as a function \(\mathcal{F}\), is implicitly a black-box representation of the paradigm.
Users of different applications which offers feature detection, segmentation, prediction, etc. are using this function without knowing what exactly happens behind the curtains. 
Further investigating \(\mathcal{F}\) we can constrain it to have a DAG computational graph, also to have the nodes of the graph arranged to lattices.
Practically it means that the perceptrons making up the layer \(l\) are strictly projecting $F_l$ their \textbf{inputs} 
$ x_l \in \mathbb{R}^N $ \emph{forward} to scalars, which if all perceptron is evaluated parallel, forms the 
\textbf{output} $y_l \in \mathbb{R}^M$, have no feedbacks and loops. 
This projection of layer $l$ can be written as $y_l = F_l(x_l)$. 
Intuitively the input of the next $(l+1)^{th}$ layer will be the output of the previous layer:
$x_{(l+1)} = y_l$.
For the sake of simplicity I excluded Recurrent Networks from the space of $\mathcal{F}$, but later the definition can be extended for vanilla recurrent networks and LSTM networks as well.

\subsection{Language} Keeping in mind, that using a general high-level language (like MATLAB) can yield poor computational efficiency, making benchmarks, testing and applications impossible.
Also considering a low-level language (like C) would distract us from the main goal, always optimizing further and further the basic algorithms -- else resulting in boilerplate codes. Either way it would make the implementation unclear for those, who did not participate in the designing of the library.
I wanted to chose a lagnuage which offers an optimal solution for this challenge, is flexible, well documented and simple enough for newbies to catch up.
Because of the support of both object-oriented and procedural approaches and offering the above, Python was the best choice.

\subsection{Design pattern} While carrying out blueprints of the implementation, I examined the Neural Networks as universal approximators in a \textsc{top-down} manner.
I have disassembled them to basic blocks, abstracting the function of each level.
Later I used these units in \textsc{bottom-up} approach to create an object-oriented hierarchic model that realize simple operations and is well defined on every level: granting a universal interface to be further extended.


\section{Inference}
Let us suppose that for a particular task an $\mathcal{F}$ is given, first what we have to understand is how input data 
$x \in \mathbb{X}$ (interchangeably $x_1$) is inferred.
First, assume that the data can be expressed as multi-dimensional matrix, like RGB pictures, audio recordings, gene maps.
Some networks preserve the spatial information i.e. feature extraction performed on images, 
while other instances operate on the whole input data i.e. processing audio samples in frequency domain.
Either way the output $y$ (or $y_L$) can be obtained by feeding $x$ to the network: 
$ y = \mathcal{F}(x)$
The core concept is that we can compose such an $\mathcal{F}$ function by applying multiple projections to $x$.
Practically that means sending input through the first layer, the second and all the way through to the last layer $L^{th}$, which output would be the value of $\mathcal{F}(x)$, the response of the network.
Therefore in terms of evaluating $\mathcal{F}(x)$ layer by layer, actually translates to a single function call, which can be unfolded to a sequence of embedded projections:
\begin{equation}\label{eq:forward}
\begin{split}
    \mathcal{F}(x) &= F_L \left( x_L \right) = F_L \left( y_{(L-1)} \right)\\
    F_L \left( y_{(L-1)} \right) = 
    F_L \left( F_{(L-1)} \left( x_{(L-1)} \right) \right) &= F_L \left( F_{(L-1)}\left( \cdots F_1(x)\right)\right)
\end{split}
\end{equation}
%
The \ref{eq:forward} equation is the most fundamental idea behind feed-forward neural networks.
As mentioned above, every layer $l$ is represented by an $F_l$. 
The most basic layers are the \emph{Fully Connected} and \emph{Activation} layers.

\subsection{Fully Connected layer} 
These layers carry out the heavy-lifting of inference by performing linear projection and translation transformations. 
The operations are following the rules of basic linear algebra, where the input $x_{FC} \in \mathbb{R}^N$ and the output $y_{FC} \in \mathbb{R}^M$ are specified as real valued vectors.
The parameters of the layer $\phi_{FC}=(W, b)$ are the corresponding weights and biases of each perceptron node in the layer forming a \emph{weight matrix} $W \in \mathbb{R}^{M \times N}$ and a \emph{bias vector} $b \in \mathbb{R}^M$ respectively.
Therefore evaluating the output of the Fully Connected layer is the defined by the following:
\begin{equation}\label{eq:FC}
\begin{split}
    y_i = \left(\sum_i W_{i,j} \cdot x_j \right) + b_i \qquad &\in \mathbb{R}\\
    y = Wx + b \qquad &\in \mathbb{R}^M
\end{split}
\end{equation}
\subsection{Activation layer} 
Nodes in activation layers are introducing non-linearity to the network, by applying the same non-linear activation function to the corresponding output of the previous layer, performing element-wise operation.
These functions are essential for the network, since they increase the numerical stability: they \emph{squeeze} or \emph{mitigate} the input preventing the network from \emph{saturation} or \emph{explosion} (numerical of course). Conventionally the following functions are applied most often as activation function:
\begin{align*}
    \mathrm{Rectified Linear Unit (ReLU) := } &max(0, x) \\
    \mathrm{Hyperbolic Tangent (TanH) := }   &tanh(x) = \frac{2}{1+e^{-2x}}-1 \\
    \mathrm{SoftPlus (SP) := }   &ln(1+e^x) \\
    \mathrm{Sigmoid (S) := }  &\frac{1}{1+e^{-x}}
\end{align*}
\emph{Note:} These functions do not have any variable parameters, therefore activation layers cannot be trained.

\section{Measuring efficiency}
If the function $\mathcal{F}$ mentioned above is given, and satisfies our needs, then we are done.
However this is usually not the case, and finding the optimal $\mathcal{F}^*$ is the main challenge targeted by many branches of Machine Learning.
Despite it was proven, that standard multilayer
feed-forward networks are capable of approximating
any measurable function to any desired degree of
accuracy \cite{hornik1989multilayer}
if the goal function $\mathcal{F}^*$ is unknown, 
or too abstract to be \emph{measurable} (i.e. telling how funny a picture is), 
we cannot utilize the universal approximator.

\subsection{Loss} 
By reformulating the objective, we can define a Loss function $\mathcal{L}:\mathcal{F}(\mathbb{X}) \rightarrow \mathbb{R}$,
which maps our candidate $\mathcal{F}$ network to a scalar field, that represents the general correctness of $\mathcal{F}$ over the space of possible inputs $\mathbb{X}$ -- the lower its value the better $\mathcal{F}$ is performing.
By doing so we may apply Machine Learning algorithms that would \emph{minimize} the Loss, therefore $\mathcal{F}$ would converge towards $\mathcal{F}^*$ implicitly.

\emph{Note:} In practical implementations $\mathcal{F}$ is not evaluated over the whole space of possible inputs, instead in the hope that a small subset of both \emph{training} and \emph{validating} samples called a mini-batch will approximate $\mathcal{L}(\mathcal{F})$ as well. Useful practices for reducing computing complexity and improving stability, and rate of convergence will be covered later.


\subsection{Supervised learning}
In cases where the parameters of $\mathcal{F}^*$ is not known but we know how it would map the input space $\mathbb{X} \rightarrow \mathbb{Y}$, e.g. which character appears on the input image $x$, then we can define a set of previously \emph{labeled} pairs of input - solution sample which could be used later on for training, and evaluating performance of the network.

\subsection{Unsupervised learning}
When no labeled dataset is available, the network still can be used for extraction of hidden structure of the unlabeled samples. Later these instances are used as density estimators, or adapted as feature extractors for larger networks.

\textbf{Generative Networks.} Training such architectures can be done by feeding networks random noise as input and training them to reproduce given samples: $\mathcal{F}:\mathbb{R}^k \rightarrow \mathbb{X}$, hence the name \emph{Generative Networks}.

\textbf{Auto-encoder Networks} The other frequently applied paradigm is setting the objective task to compress the input sample into a $h \in \mathbb{R}^k$ hidden representation vector that is able to preserve the key information about the original input, and either by symmetric (Restrictive Boltzmann Machines) or independent (Deep Belief networks) operations decompress the data.

In both cases hyper-parameter $k$ is intuitively the number of unlabeled features in the \emph{latent space} that the network will be able to categorize, i.e. correlation between different color intensities on pictures taken of stained brain samples.


\section{Adjusting parameters}
Generally speaking, applying Machine Learning algorithms boils down to the process of iteratively altering parameters $\phi$ of $\mathcal{F}$ to optimize the Loss.
Once $\mathcal{L}(\mathcal{F})$ is obtained, we can evaluate how changing the parameters would influence it -- evaluate the gradient $\nabla_\phi \mathcal{F}$ of the parameters  with regards to $\mathcal{L}$.

\subsection{Gradient Descent}
Updating $\phi$ by descending on the gradient slope with a small step size $\epsilon$ will decrease $\mathcal{L}$.
Enucleating the general form of Gradient Descent depends on the architecture of the network, but there is a main concept for doing so, called Backpropagation described by Werbos \emph{et al.} \cite{werbos1994roots}.

\subsection{Backpropagation} 
Though networks can vary in shape and function, the representation of the succeeding layers, evaluated by embedded functions described in \ref{eq:forward} is a common feature. Each of these layers serve as nodes of the computational graph of the network. The type of the layer determines whether it can be updated or not: for \emph{Fully Connected} layers $\nabla_\phi F_{FC}$ is well defined, explained in the following example.

\paragraph{Toy example.} Assume $\mathcal{L}$ is given, and we have a three-layer fully connected network $L=3$, which maps a $N$ dimensional input vector $x$ to a $M$ dimensional output vector $y$, namely $\mathcal{F}:\mathbb{R}^N \rightarrow \mathbb{R}^M$. The constraint on the parameters are:
\begin{itemize}
    \item[] $W_1$ of the first layer must have $N$ columns
    \item[] $W_3$ and the bias $b_3$ of the last layer must have $M$ rows, dimensions respectively.
    \item[] Number of rows of $W_l$ must match $dim(b_l)$
    \item[] Number of columns of $W_l$ must match $dim(b_{(l-1)})$
\end{itemize} 
Then the evaluation unfolded would look like the following: 
$$y = \mathcal{F}(x) = F_3(F_2(F_1(x))) = W_3(W_2(W_1(x)+b_1)+b_2)+b_3$$
For further usage and simplicity, I would like to fix these numbers.
Let $\mathcal{F}$ be a network with the following \emph{shape}: $\left[5, 4, 3\right] $
meaning that in each layer there are 5, 4, 3 neurons respectively $M=3$, all connected to the previous layer. The width of the input layer is not yet defined, let be $N=10$.
\section{Derivation} In the above evaluating of $\nabla_\phi \mathcal{F}$ can be done in two ways, namely by numerical approximation, or by analytical derivation, in the following I will discuss both.

\subsection{Numeric} Evaluating the numerical gradient (or difference) is an elementary, yet powerful operation, in which we would \emph{perturbate}, or modify one parameter $\phi$ of our system $\mathcal{F}$ at once.
That is done by first adding $\phi^+$ and after subtracting $\phi^-$ a little amount $d\phi$ from the original $\phi$ and evaluate $\mathcal{L}^{\pm}=\mathcal{L}(\mathcal{F}_{\phi^{\pm}})$, namely the \emph{Loss} of the system in the modified state, yielding the numerical gradient in the following equation:

\begin{equation} \label{eq:numgrad}
    \frac{d\mathcal{L}(\mathcal{F})}{d\phi} = 
    \frac{\mathcal{L}^+ - \mathcal{L}^-}{2 d\phi} = 
    \frac{\mathcal{L}(\mathcal{F}_{\phi^+}) - \mathcal{L}(\mathcal{F}_{\phi^-})}{2 d\phi}
\end{equation}

\paragraph{Summary.} In a nutshell value of $\frac{d\mathcal{L}(\mathcal{F})}{d\phi}$ tells how changing the parameter $\phi$ by $d\phi$ would change the performance of the network. If it is positive then updating $\mathcal{F}$ by adding $d\phi$ to $\phi$ would result in higher Loss value, which is the opposite of our goal, so we just subtract it, if it is negative, than trivially we should add $d\phi$ to $\phi$ since it is making some good progress.

\subsection{Complexity} 
Though letting the computer do the hard work seems to be a good idea, it worths considering that the simple method above will be applied to every $\phi$ of $\mathcal{L}$.
It means that for the network in the toy example, we need to evaluate $\mathcal{L}(\mathcal{F})$ two times for each parameter in 
the weight matrices and the bias vectors of the network, totaling in 
$$\#(\phi) = 2\times\sum_{i=1}^3 N_{(i-1)}\cdot N_i + N_i = 2\times(10\cdot 5 + 5 ... + 4\cdot 3 + 3) = 188$$
Even if $\mathcal{F}$ is approximated by using $k$-sized mini-batches for evaluation it is still a computationally very expensive function, because the inference would result in the following number of operations of addition and multiplication:
$$\#(\mathrm{operations}) = k\times\sum_{i=1}^3 2\cdot N_{(i-1)}\cdot N_i + N_i = k \times 176$$
Therefore approximated with $k=10$ mini-batches would a single parameter update of a very tiny network would require total operations of:
\begin{equation}
    \#(\mathrm{total})=\#(\phi) \times k \times \#(\mathrm{operations}) = 188 \cdot 10 \cdot 176  = 330880
\end{equation}
Because both $\#(\phi)$ and $\#(\mathrm{operations})$ has complexity of $\mathcal{O}(N^2)$, one update will yield complexity of 
\begin{equation}
    \#(\mathrm{total})=\mathcal{O}(N^4)
\end{equation}
We can see that even for a shallow and relatively small network (industrial AI networks has billions of parameters, and uses much larger batches) described in the toy example the method is really costly.
That encourages us to derive our differentials on paper first, and use \emph{numerical gradient approximation} for checking our solution. Using \emph{Gradient Check} is essential when implementing new architectures, because it is a very efficient tool for debugging in comparison with updating. The method in a few words is about setting an error rate $\epsilon$, and decrease $d\phi$ until the numeric solution does not match the analytic solution with $1-\epsilon$ significance. If the analytic solution is incorrect the cycle will not terminate.

\subsection{Analytic}
Deriving the update by hand requires basic knowledge in calculus extended to multivariate cases, though since the operations are elementary, in general we must understand only three basic definitions to do so. 
Some formality before starting: we have three independent variables $x$, $y$, $z$, and functions $f$, $g$, $h$. The result of operations performed on variables, i.e. $x+2y$ can be represented by a function $f=x+2y$. 
If the value depends on a variable then it can be written explicitly, passing the variable as \emph{the argument} of the function $f(x,y)=x+2y$. 
For the sake of simplicity assume that the variables are not general objects from an abstract space, they are only real values: $x,y,z\in \mathbb{R}$. However the following description could be extended for the above-mentioned variables as well. For any one-dimensional function $f(x):\mathbb{R}\rightarrow\mathbb{R}$ we say that the value represented by the function depends on the variable by the extent of its derivative. The derivative (or differential) of the function can be seen as an ideal case of \ref{eq:numgrad} where the perturbation would approach zero, namely:
\begin{equation}
    \frac{\partial f}{\partial x} = \lim_{dx\rightarrow 0}\frac{df(x)}{dx}= \lim_{dx\rightarrow 0} \frac{f(x+dx)-f(x-dx)}{2dx}
\end{equation}
\paragraph{Multiplication rule.}
Consider a value $x\cdot y \cdot z$ represented by $f(x,y,z)$. $f$ is now depending on three variables, we can define the measure of this dependency on one variable by the formal equation:
\begin{equation*}
\begin{split}
    \frac{\partial f}{\partial x} = \lim_{dx\rightarrow 0} \frac{f(x+dx,y,z)-f(x-dx,y,z)}{dx}
    &= \lim_{dx\rightarrow 0} \frac{((x+dx)\cdot y \cdot z)-((x-dx)\cdot y \cdot z)}{2dx} \\
    = \lim_{dx\rightarrow 0} \frac{(x+dx)-(x-dx)}{2dx} y z&= \lim_{dx\rightarrow 0} \frac{2dx}{2dx} yz= yz
\end{split}
\end{equation*}
We can apply the same method for each variable, the result will be the elements of the \emph{gradient} $\nabla f$
\begin{equation}
    \frac{\partial f}{\partial x} = y z \qquad
    \frac{\partial f}{\partial y} = x z \qquad
    \frac{\partial f}{\partial z} = x y 
\end{equation} 
The important thing to understand that in a computational graph, a multiplicative node, which takes $N$ arbitrary parameters (or arguments), will have a \emph{partial derivative} for each variable its output is depending on. In general if these derivatives are represented as a vector, then it is called the gradient $\nabla f$ of $f$. Also the value of the derivative will be the product of all variables except the one of which we are computing the influence of on the output.

\paragraph{Addition.}
Consider a value $x\cdot y + x\cdot z$ represented by $g(x,y,z)$.
The change of $g$ with respect to $x$ is defined with the following shortened equation:
\begin{equation}
    \frac{\partial g}{\partial x} = \lim_{dx\rightarrow 0} \frac{((x+dx)y +(x+dx)z)-((x-dx)y +(x-dx)z)}{dx}=y+z
\end{equation}
Notice that -- in the terms of computational graphs -- if a node contributes to other different operations (namely $x\cdot y$ and $x\cdot z$), 
than the derivative of each occurrence in \emph{later} values will be summed up. 

\paragraph{Chain rule.}
Let $f(x)=2x+3$ and $g(f)=5f$. Suppose that we would like to know the derivative of $g$ with respect to $x$.
At first we cannot do so, but there are two options: in the hope that substituting the value represented by $f$ into $g$ would not make the equation too complex we can unroll the references and rewrite $g(x)=5\cdot(2x + 3)$, or we could use the chain rule:
$$
    \frac{\partial g}{\partial x} = \lim_{dx \rightarrow 0} \frac{g(f(x+dx)) - g(f(x-dx))}{2dx}
$$

\begin{center}
    Assume that $f(x+dx)-f(x-dx) \neq 0$.
\end{center}

$$
    \lim_{dx \rightarrow 0} \frac{g(f(x+dx)) - g(f(x-dx) ) }{f(x+dx)-f(x-dx)} \cdot \frac{f(x+dx)-f(x-dx)}{2dx}
$$

\begin{equation}\label{eq:chain}
    \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial x}
\end{equation}

Which is a formula of the products of partial derivative of $g$, that treats $f$ like a variable, and $f$ explicitly operating on variable $x$.
After this we can interchangeably use function values and variables with a constraint that at a point, in an arbitrary depth there must be a real variable.
\emph{Note:} the statement above stands for computations with \emph{Acyclic Graph}, meaning that there should not be any feedbacks or loops -- no definitions like $f(g), g(f)$ or $f(f)$.
We will see that these rules play a very fundamental role in training networks.

