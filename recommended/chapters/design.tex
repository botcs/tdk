\chapter{Design and Implementation}

After researching literature on building libraries from scratch, and analyzing pet project source-codes \cite{convnetjs, gibianskysource} and implementations of industrial APIs \cite{TF, caffe, torch}
I concluded that the best way to acquire that knowledge is to build my own Deep Learning framework.
This way I had a chance to understand why novel solutions in Machine Learning are formed the way they are.
Also gained insight into what main paradigms are popular libraries based on, such as \emph{computational graphs, parallel distributed processing}, 
and what trade-offs can be made between \emph{computational cost} and \emph{memory usage}, between robustness and plasticity.
Most thankfully, by starting from scratch I have faced challenges of situations when theoretical formulas had to be translated into exact working code, which roughly speaking is the hardest part of modern science.

\paragraph{Goals.} My main objectives when writing code was the following:
\begin{itemize}
    \item[] to make such a library that is able to be extended further
    \item[] open-source, so it can be forked by anyone interested in developing it
    \item[] to make it modular therefore its usage independent of the task
    \item[] use as least possible technical tricks for sake of simplicity
    \item[] to stay as close to pure mathematical formulation of the classical paradigms as possible
    \item[] put emphasis on ease of use and understanding
\end{itemize}
\paragraph{Disclaimer.} Apart from \textbf{NumPy} and its complementary package \textbf{SciPy}, no external libraries and dependencies are built in the implementation. 
I want to emphasize that this work was not written to compete with contemporary state-of-the-art frameworks, rather to help perceive the general ideas behind novel researches, and to encourage interested fellows to carry out researches on their own.

\section{Choice of design pattern and language}

The formulas of classical neural networks, whose nodes are organized into lattices forming a Directed Acyclic Graph, shows soft variance in contents of independent sources. Still their common point that they rely on fundamentals (basics?) of vector algebra and functional analysis, therefore considering array represented general mapping functions, as their core building units is a paradigm which would not interfere with the current formal and informal descriptions of deep learning.

\subsection{Disassembling a universal approximator}
Thinking of a neural net as a function \(\mathcal{F}\), is implicitly a black-box representation of the paradigm.
Users of different applications which offers feature detection, segmentation, prediction, etc. are using this function without knowing what exactly happens behind the curtains. 
Further investigating \(\mathcal{F}\) we can constrain it to have a DAG computational graph, also to have the nodes of the graph arranged to lattices.
Practically it means that the perceptrons making up the layer \(l\) are strictly projecting $F_l$ their \textbf{inputs} 
$ x_l \in \mathbb{R}^N $ \emph{forward} to scalars, which if all perceptron is evaluated parallel forms the 
\textbf{output} $y_l \in \mathbb{R}^M$, have no feedbacks and loops. 
This projection of layer $l$ can be written as $y_l = F_l(x_l)$. 
Intuitively the input of the next $(l+1)^{th}$ layer will be the output of the previous layer:
$x_{(l+1)} = y_l$.
For the sake of simplicity I excluded Recurrent Networks from the space of $\mathcal{F}$, but later the definition can be extended for vanilla recurrent networks and LSTM networks as well.

\subsection{Language} Keeping in mind, that using a general high-level language (like MATLAB) can yield poor computational efficiency, making benchmarks, testing and applications impossible.
Also considering a low-level language (like C) would distract us from the main goal, always optimizing further and further the basic algorithms -- else resulting in boilerplate codes. Either way it would make the implementation unclear for those, who did not participate in the designing of the library.
I wanted to chose a lagnuage which offers an optimal solution for this challenge, is flexible, well documented and simple enough for newbies to catch up.
Because of the support of both object-oriented and procedural approaches and offering the above, Python was the best choice.

\subsection{Design pattern} While carrying out blueprints of the implementation, I examined the Neural Networks as universal approximators in a \textsc{top-down} manner.
I have disassembled them to basic units, abstracting the function of each level.
Later I used these units in \textsc{bottom-up} approach to create an object-oriented hierarchic model that realize simple operations and is well defined on every level: granting a universal interface to be further extended.


\section{Inference}
Let us suppose that for a particular object an $\mathcal{F}$ is given, first what we have to understand is how input data 
$x \in \mathbb{X}$ (interchangeably $x_1$) is inferred.
First, assume that the data can be expressed as multi-dimensional matrix, like RGB pictures, audio recordings, gene maps.
Some network preserves the spatial information i.e. feature extraction performed on images, 
while other instances operate on the whole input data i.e. processing audio samples in frequency domain.
Either way the output $y$ (or $y_L$) can be obtained by feeding $x$ to the network: 
$ y = \mathcal{F}(x)$
The core concept is that we can compose such an $\mathcal{F}$ function by applying multiple projections to $x$.
Practically that means sending input through the first layer, the second and all the way through to the last layer $L^{th}$, which output would be the value of $\mathcal{F}(x)$, the response of the network.
Therefore in terms of evaluating $\mathcal{F}(x)$ layer by layer actually translates to a recursive function call, which can be unfolded to a sequence of embedded projections:
\begin{equation}\label{eq:forward}
\begin{split}
    \mathcal{F}(x) &= F_L \left( x_L \right) = F_L \left( y_{(L-1)} \right)\\
    F_L \left( y_{(L-1)} \right) = 
    F_L \left( F_{(L-1)} \left( x_{(L-1)} \right) \right) &= F_L \left( F_{(L-1)}\left( \cdots F_1(x)\right)\right)
\end{split}
\end{equation}
%
The \ref{eq:forward} equation is the most fundamental idea behind feed-forward neural networks.
As mentioned above, every layer $l$ are represented by $F_l$. 
The most basic layers are the \emph{Fully Connected} and \emph{Activation} layers.

\subsection{Fully Connected layer} 
These layers carry out the heavy-lifting of inference by performing linear projection and translation transformations. 
The operations are following the rules of basic linear algebra, where the input $x_{FC} \in \mathbb{R}^N$ and the output $y_{FC} \in \mathbb{R}^M$ are specified as real valued vectors.
The parameters of the layer $\phi_{FC}=(W, b)$ are the corresponding weights and biases of each perceptron node in the layer forming a \emph{weight matrix} $W \in \mathbb{R}^{M \times N}$ and a \emph{bias vector} $b \in \mathbb{R}^M$ respectively.
Therefore evaluating the output of the Fully Connected layer is the defined by the following:
\begin{equation}\label{eq:FC}
\begin{split}
    y_i = \left(\sum_i W_{i,j} \cdot x_j \right) + b_i \qquad &\in \mathbb{R}\\
    y = Wx + b \qquad &\in \mathbb{R}^M
\end{split}
\end{equation}
\subsection{Activation layer} 
Nodes in activation layers are introducing non-linearity to the network, by applying the same non-linear activation function to the corresponding output of the previous layer, performing element-wise operation.
These functions are essential for the network, since they increase the numerical stability of the preceding they \emph{squeeze} or \emph{mitigate} the input preventing the network from \emph{saturation} or \emph{explosion} (numerical of course). Conventionally the following functions are applied most often as activation function:
\begin{align*}
    \mathrm{Rectified Linear Unit (ReLU) := } &max(0, x) \\
    \mathrm{Hyperbolic Tangent (TanH) := }   &tanh(x) = \frac{2}{1+e^{-2x}}-1 \\
    \mathrm{SoftPlus (SP) := }   &ln(1+e^x) \\
    \mathrm{Sigmoid (S) := }  &\frac{1}{1+e^{-x}}
\end{align*}
\emph{Note:} These functions do not have any variable parameters, therefore activation layers cannot be trained.

\section{Measuring efficiency}
If the function $\mathcal{F}$ mentioned above is given, and satisfy our needs then we are done.
However this is usually not the case, and finding the optimal $\mathcal{F}^*$ is the main challenge targeted by many branches of Machine Learning.
Despite it has proven, that standard multilayer
feed-forward networks are capable of approximating
any measurable function to any desired degree of
accuracy \cite{hornik1989multilayer}
if the goal function $\mathcal{F}^*$ is unknown, 
or too abstract to be \emph{measurable} (i.e. telling how funny a picture is), 
we cannot utilize the universal approximator.

\subsection{Loss} 
By reformulating the objective, we can define a Loss function $\mathcal{L}:\mathcal{F}(\mathbb{X}) \rightarrow \mathbb{R}$,
which maps our candidate $\mathcal{F}$ network to a scalar field, that represents the general incorrectness of $\mathcal{F}$ over the space of possible inputs $\mathbb{X}$.
By doing so we may apply Machine Learning algorithms that would \emph{minimize} the Loss, therefore $\mathcal{F}$ would converge towards $\mathcal{F}^*$ implicitly.

\emph{Note:} In practical implementations $\mathcal{F}$ is not evaluated over the whole space of possible inputs, instead in the hope that a small subset of both \emph{training} and \emph{validating} samples called a mini-batch will approximate $\mathcal{L}(\mathcal{F})$ as well. Useful practices for reducing computing complexity and improving stability, and rate of convergence will be covered later.

\section{Adjusting parameters}
Generally speaking, applying Machine Learning algorithms boils down to the process of iteratively altering parameters $\phi$ of $\mathcal{F}$ to optimize the Loss.
Once $\mathcal{L}(\mathcal{F})$ is obtained, we can evaluate how changing the parameters would influence it -- evaluate the gradient of the parameters $\nabla \phi$ with regards to $\mathcal{L}$.

\subsection{Supervised learning}
In cases where the parameters of $\mathcal{F}^*$ is not known but we know how it would map the input space $\mathbb{X} \rightarrow \mathbb{Y}$, i.e. which character appears on the input image $x$, then we can define a set of previously \emph{labeled} pairs of input - solution sample which could be used later on for training, and evaluating performance of the network.

\subsection{Unsupervised learning}
When no labeled dataset is available, the network still can be used for extraction of hidden structure of the unlabeled samples. Later these instances are used as density estimators, or adapted as feature extractors for larger networks.

\textbf{Generative Networks.} Training such architectures can be done by feeding networks random noise as input and training them to reproduce given samples: $\mathcal{F}:\mathbb{R}^k \rightarrow \mathbb{X}$, hence the name \emph{Generative Networks}.

\textbf{Auto-encoder Networks} The other frequently applied paradigm is setting the objective task to compress the input sample into a $h \in \mathbb{R}^k$ hidden representation vector that is able to preserve the key information about the original input, and either by symmetric (Restrictive Boltzmann Machines) or independent (Deep Belief networks) operations decompress the data.

In both cases hyper-parameter $k$ is intuitively the number of unlabeled features in the \emph{latent space} that the network will be able to categorize, i.e. correlation between different color intensities on pictures taken of stained brain samples.

\subsection{Gradient Descent}
Updating $\phi$ by descending on the gradient steep with a small step size $\epsilon$ will decrease $\mathcal{L}$.
Enucleating the general form of Gradient Descent depends on the architecture of the network, but there is a main concept for doing so, called Backpropagation invented by Werbos \emph{et al.} \cite{werbos1994roots}.

\subsection{Backpropagation} 
Though networks can vary in shape and function, the representation of the succeeding layers, evaluated by embedded functions described in \ref{eq:forward} is a common feature. Each of these layers serve as nodes of the computational graph of the network. The type of the layer determines whether it can be updated or not: for \emph{Fully Connected} layers $\nabla \phi_{FC}$ is well defined, explained in the following example.

\paragraph{Toy example.} Assume $\mathcal{L}$ is given, and we have a three-layer fully connected network $L=3$, which maps a $N$ dimensional input vector $x$ to a $M$ dimensional output vector $y$, namely $\mathcal{F}:\mathbb{R}^N \rightarrow \mathbb{R}^M$. The constraint on the parameters are:
\begin{itemize}
    \item[] $W_1$ of the first layer must have $N$ columns
    \item[] $W_3$ and the bias $b_3$ of the last layer must have $M$ rows, dimensions respectively.
    \item[] Number of rows of $W_l$ must match $dim(b_l)$
    \item[] Number of columns of $W_l$ must match $dim(b_{(l-1)})$
\end{itemize} 
Then the evaluation unfolded would look like the following: 
$$y = \mathcal{F}(x) = F_3(F_2(F_1(x))) = W_3(W_2(W_1(x)+b_1)+b_2)+b_3$$
For further usage and simplicity, I would like to fix these numbers.
Let $\mathcal{F}$ be a network with the following \emph{shape}: $5, 4, 3$
meaning that in each layer there are 5, 4, 3 neurons respectively $M=3$, all connected to the previous layer. The width of the input layer is not yet defined, let be $N=10$.
\section{Derivation} In the above evaluating of $\nabla \phi$ can be done in two ways, namely by numerical approximation, or by analytical derivation, in the following I will discuss both.

\subsection{Numerical} Evaluating the numerical gradient (or difference) is an elementary, yet powerful operation, in which we would \emph{perturbate}, or modify one parameter $\phi$ of our system $\mathcal{F}$ at once.
That is done by first adding $\phi^+$ and after subtracting $\phi^-$ a little amount $d\phi$ from the original $\phi$ and evaluate $\mathcal{L}^{\pm}=\mathcal{L}(\mathcal{F}_{\phi^{\pm}})$, namely the \emph{Loss} of the system in the modified state, yielding the numerical gradient in the following equation:

\begin{equation}
    \frac{d\mathcal{L}(\mathcal{F})}{d\phi} = 
    \frac{\mathcal{L}^+ - \mathcal{L}^-}{2 d\phi} = 
    \frac{\mathcal{L}(\mathcal{F}_{\phi^+}) - \mathcal{L}(\mathcal{F}_{\phi^-})}{2 d\phi}
\end{equation}

\subsection{Complexity} 
Though letting the computer do the hard work seems to be a good idea, it worths considering that the simple method above will be applied to every $\phi$ of $\mathcal{L}$.
It means that for the network in the toy example, we need to evaluate $\mathcal{L}(\mathcal{F})$ two times for each parameter in 
the weight matrices and the bias vectors of the network, totaling in 
$$\#(\phi) = 2\times\sum_{i=1}^3 N_{(i-1)}\cdot N_i + N_i = 2\times(10\cdot 5 + 5 ... + 4\cdot 3 + 3) = 188$$
Even if $\mathcal{F}$ is approximated by using $k$-sized mini-batches for evaluation it is still a computationally very expensive function, because the inference would result in the following number of operations of addition and multiplication:
$$\#(\mathrm{operations}) = k\times\sum_{i=1}^3 2\cdot N_{(i-1)}\cdot N_i + N_i = k \times 176$$
Therefore approximated with $k=10$ mini-batches would a single parameter update of a very tiny network would require total operations of:
\begin{equation}
    \#(\mathrm{total})=\#(\phi) \times k \times \#(\mathrm{operations}) = 188 \cdot 10 \cdot 176  = 330880
\end{equation}
Because both $\#(\phi)$ and $\#(\mathrm{operations})$ has complexity of $\mathcal{O}(N^2)$, one update will yield complexity of 
\begin{equation}
    \#(\mathrm{total})=\mathcal{O}(N^4)
\end{equation}

Which rather encourages us to derive our differentials on paper first, and use numerical gradient approximation for checking whether our implementation is correct.
\subsection{Analytical}
requires basic knowledge in calculus, though since the operations are very easy, in general it would require three theorems to 
