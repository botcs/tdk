\chapter{Design and Implementation}

After researching literature on building libraries from scratch, and analyzing pet project source-codes \cite{convnetjs, gibianskysource} and implementations of industrial APIs \cite{TF, caffe, torch}
I concluded that the best way to acquire that knowledge is to build my own Deep Learning framework.
This way I had a chance to understand why novel solutions in Machine Learning are formed the way they are.
Also gained insight into what main paradigms are popular libraries based on, such as \emph{computational graphs, parallel distributed processings}, 
and what trade-offs can be made between \emph{computational cost} and \emph{memory usage}, between robustness and plasticity.
Most thankfully, by starting from scratch I have faced challenges of situations when theoretical formulas had to be translated into exact working code, which roughly speaking is the hardest part of modern science.

\textbf{Goals.} My main intentions when started writing code was the following:
\begin{itemize}
    \item[] to make such a library that is able to be extended further
    \item[] open-source, so it can be forked by anyone interested in developing it
    \item[] to make it modular therefore its usage independent of the task
    \item[] use as least possible technical tricks for sake of simplicity
    \item[] to stay as close to pure mathematical formulation of the classical paradigms as possible
    \item[] put emphasis on ease of use and understanding
\end{itemize}

\textbf{Disclaimer.} Apart from \textbf{NumPy} and its complementary package \textbf{SciPy}, no outer libraries and dependecies are built in the implementation. 
I want to emphasize that this work was not written to compete with conemporary state-of-the-art frameworks, rather to help perceive the general ideas behind novel researches, and to encourage interested fellows to carry out researches on their own.

\section{Choice of design pattern and language}

The formulas of classical neural networks, whose nodes are organized into lattices forming a Directed Acyclic Graph, shows soft variance in contents of independent sources. Still their common point that they rely on fundaments of vector algebra and functional analysis, therefore considering array representated general mapping functions, as their core building units is a paradigm which would not interfere with the current formal and informal descriptions of deep learning.

Keeping in mind, that using a general high-level language (like MATLAB) can yield poor computational efficiency, making benchmarks, testing and applications impossible.
Also considering a low-level language (like C) would distract us from the main goal, always optimizing further and further the basic algorithms -- else resulting in boilerplate codes. Either way it would make the implementation unclear for those, who did not participate in the designing of the library.
I wanted to chose a lagnuage which offers an optimal solution for this challenge, is flexible, well documented and simple enough for newbies to catch up.
Because of the support of both object-oriented and procedural approaches and offering the above, Python was the best choice.

While carrying out blueprints of the implementation, I examined the Neural Networks as universal approximators in a \textsc{top-down} manner.
I have disassembled them to basic units, abstracting the function of each level.
Later I used these units in \textsc{bottom-up} approach to create a model that realize simple operations, and is well defined on every level: granting a universal interface to be further extended.

\subsection{Disassembling a universal approximator}
%\in \mathfrak{F}
Thinking of a neural net as a function \(\mathcal{F}\), is implicitly a black-box representation of the paradigm.
Users of different applications which offers feature detection, segmentation, prediction, etc. are using this function without knowing what exactly happens behind the curtains. 
Further investigating \(\mathcal{F}\) we can constrain it to have a DAG computational graph, also to have the nodes of the graph arranged to lattices.
Practically it means that the perceptrons making up the layer \(l\) are strictly projecting $F_l$ their \textbf{inputs} 
$ x_l \in \mathbb{R}^N $ \emph{forward} to scalars, which if all perceptron is evaluated parallel forms the 
\textbf{output} $y_l \in \mathbb{R}^M$, have no feedbacks and loops. 
This projection of layer $l$ can be written as $y_l = F_l(x_l)$. 
Intuitively the input of the next $(l+1)^{th}$ layer will be the output of the previous layer:
$x_{(l+1)} = y_l$.
For the sake of simplicity I excluded Recurrent Networks from the space of $\mathcal{F}$, but later the definition can be extended for vanilla recurrent networks and LSTM networks as well.

\paragraph{Inference.}
Let us suppose that for a particular object an $\mathcal{F}$ is given, first what we have to understand is how input data $x$ (interchangebly $x_1$) is infered.
First, assume that the data can be expressed as multi-dimensional matrix, like RGB pictures, audio recordings, gene maps.
Some network preserves the spatial information i.e. feature extraction performed on images, 
while other instances operate on the whole input data i.e. processing audio samples in frequency domain.
Either way the output $y$ (or $y_L$) can be obtained by feeding $x$ to the network: 
$ y = \mathcal{F}(x)$
The core concept is that we can compose such an $\mathcal{F}$ function by applying multiple projections to $x$.
Practically that means sending input through the first layer, the second and all the way through to the last layer $L^{th}$, which output would be the value of $\mathcal{F}(x)$, the response of the network.
Therefore in terms of evaluating $\mathcal{F}(x)$ layer by layer actually translates to a recursive function call, which can be unfolded to a sequence of embedded projections:
\begin{equation}
\begin{split}
    \mathcal{F}(x) & = F_L(x_L) \\
    F_L(x_L) & = F_L(y_{(L-1)}) = F_L(F_{L-1}(x_{(L-1)})) = F_L(F_{L-1}(\cdots F_1(x)))
\end{split}
\end{equation}
    



