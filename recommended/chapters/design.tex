\chapter{Design and Implementation}

After researching literature on building libraries from scratch, and analyzing pet project source-codes \cite{convnetjs, gibianskysource} and implementations of industrial APIs \cite{TF, caffe, torch}
I concluded that the best way to acquire that knowledge is to build my own Deep Learning framework.
This way I had a chance to understand why novel solutions in Machine Learning are formed the way they are.
Also gained insight into what main paradigms are popular libraries based on, such as \emph{computational graphs, parallel distributed processings}, 
and what trade-offs can be made between \emph{computational cost} and \emph{memory usage}, between robustness and plasticity.
Most thankfully, by starting from scratch I have faced challenges of situations when theoretical formulas had to be translated into exact working code, which roughly speaking is the hardest part of modern science.

\textbf{Goals.} My main intentions when started writing code was the following:
\begin{itemize}
    \item[] to make such a library that is able to be extended further
    \item[] open-source, so it can be forked by anyone interested in developing it
    \item[] to make it modular therefore its usage independent of the task
    \item[] use as least possible technical tricks for sake of simplicity
    \item[] to stay as close to pure mathematical formulation of the classical paradigms as possible
    \item[] put emphasis on ease of use and understanding
\end{itemize}

\textbf{Disclaimer.} Apart from \textbf{NumPy} and its complementary package \textbf{SciPy}, no outer libraries and dependecies are built in the implementation. 
I want to emphasize that this work was not written to compete with conemporary state-of-the-art frameworks, rather to help perceive the general ideas behind novel researches, and to encourage interested fellows to carry out researches on their own.

\section{Choice of design pattern and language}

The formulas of classical neural networks, whose nodes are organized into lattices forming a Directed Acyclic Graph, shows soft variance in contents of independent sources. Still their common point that they rely on fundaments of vector algebra and functional analysis, therefore considering array representated general mapping functions, as their core building units is a paradigm which would not interfere with the current formal and informal descriptions of deep learning.

Keeping in mind, that using a general high-level language (like MATLAB) can yield poor computational efficiency, making benchmarks, testing and applications impossible.
Also considering a low-level language (like C) would distract us from the main goal, always optimizing further and further the basic algorithms -- else resulting in boilerplate codes. Either way it would make the implementation unclear for those, who did not participate in the designing of the library.
I wanted to chose a lagnuage which offers an optimal solution for this challenge, is flexible, well documented and simple enough for newbies to catch up.
Because of the support of both object-oriented and procedural approaches and offering the above, Python was the best choice.

